{"cells":[{"cell_type":"markdown","metadata":{"id":"VEYe67K6E6j0","cell_id":"c6969f13de344254a70263916f490963","deepnote_cell_type":"markdown"},"source":"## **Grasp Candidate Sampling**\n","block_group":"1cfcb7fe86914498b171684e2978bfde"},{"cell_type":"code","metadata":{"id":"v5OrhpSmxkGH","source_hash":"7bf479e2","execution_start":1742050102779,"execution_millis":712,"execution_context_id":"4e161541-39d1-49dc-94e1-3fe79f9acfd5","cell_id":"f76a0bf3b669429ca3095b53775eb19a","deepnote_cell_type":"code"},"source":"import numpy as np\nfrom pydrake.all import (\n    AddMultibodyPlantSceneGraph,\n    Box,\n    DiagramBuilder,\n    MeshcatVisualizer,\n    MeshcatVisualizerParams,\n    Parser,\n    PointCloud,\n    Rgba,\n    RigidTransform,\n    RotationMatrix,\n    StartMeshcat,\n)\nfrom scipy.spatial import KDTree\n\nfrom manipulation import running_as_notebook\nfrom manipulation.meshcat_utils import AddMeshcatTriad\nfrom manipulation.mustard_depth_camera_example import MustardPointCloud\nfrom manipulation.scenarios import AddMultibodyTriad\nfrom manipulation.utils import ConfigureParser","block_group":"a8606db359f4435184fdfc423e9445be","execution_count":1,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"6d42057f","execution_start":1742050103540,"execution_millis":1,"execution_context_id":"4e161541-39d1-49dc-94e1-3fe79f9acfd5","cell_id":"a82e0b8356cd4ca7a72f9beaa8fe031d","deepnote_cell_type":"code"},"source":"# Start the visualizer.\nmeshcat = StartMeshcat()","block_group":"bd2de10cf0bc4632b4f5c61c65d1c352","execution_count":2,"outputs":[{"name":"stderr","text":"INFO:drake:Meshcat listening for connections at https://533dba22-ab7f-4205-8f29-33667a0de43c.deepnoteproject.com/7000/\n","output_type":"stream"},{"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Meshcat URL: <a href='https://533dba22-ab7f-4205-8f29-33667a0de43c.deepnoteproject.com/7000/' target='_blank'>https://533dba22-ab7f-4205-8f29-33667a0de43c.deepnoteproject.com/7000/</a>"},"metadata":{},"output_type":"display_data"}],"outputs_reference":"dbtable:cell_outputs/e8357763-74c6-45c0-9ffa-2c5e4d0e2125","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"fb18cc56","execution_start":1742050103728,"execution_millis":1201,"execution_context_id":"4e161541-39d1-49dc-94e1-3fe79f9acfd5","cell_id":"e829d7960512428d9f5a4b9a2047f55f","deepnote_cell_type":"code"},"source":"# Basic setup\npcd = MustardPointCloud(normals=True, down_sample=False)\n\nmeshcat.SetProperty(\"/Background\", \"visible\", False)\nmeshcat.SetObject(\"cloud\", pcd, point_size=0.001)\n\n\ndef setup_grasp_diagram(draw_frames=True):\n    builder = DiagramBuilder()\n    plant, scene_graph = AddMultibodyPlantSceneGraph(builder, time_step=0.001)\n    parser = Parser(plant)\n    ConfigureParser(parser)\n    parser.AddModelsFromUrl(\"package://manipulation/schunk_wsg_50_welded_fingers.sdf\")\n    if draw_frames:\n        AddMultibodyTriad(plant.GetFrameByName(\"body\"), scene_graph)\n    plant.Finalize()\n\n    MeshcatVisualizer.AddToBuilder(builder, scene_graph, meshcat)\n    diagram = builder.Build()\n    context = diagram.CreateDefaultContext()\n\n    return plant, scene_graph, diagram, context\n\n\ndef draw_grasp_candidate(X_G, prefix=\"gripper\", draw_frames=True, refresh=False):\n    if refresh:\n        meshcat.Delete()\n    builder = DiagramBuilder()\n    plant, scene_graph = AddMultibodyPlantSceneGraph(builder, time_step=0.001)\n    parser = Parser(plant)\n    ConfigureParser(parser)\n    gripper = parser.AddModelsFromUrl(\n        \"package://manipulation/schunk_wsg_50_welded_fingers.sdf\"\n    )\n    plant.WeldFrames(plant.world_frame(), plant.GetFrameByName(\"body\"), X_G)\n    if draw_frames:\n        AddMultibodyTriad(plant.GetFrameByName(\"body\"), scene_graph)\n    plant.Finalize()\n\n    params = MeshcatVisualizerParams()\n    params.prefix = prefix\n    meshcat_vis = MeshcatVisualizer.AddToBuilder(builder, scene_graph, meshcat, params)\n    diagram = builder.Build()\n    context = diagram.CreateDefaultContext()\n    diagram.ForcedPublish(context)\n\n\ndef compute_sdf(pcd, X_G, visualize=False):\n    plant, scene_graph, diagram, context = setup_grasp_diagram()\n    plant_context = plant.GetMyContextFromRoot(context)\n    scene_graph_context = scene_graph.GetMyContextFromRoot(context)\n    plant.SetFreeBodyPose(plant_context, plant.GetBodyByName(\"body\"), X_G)\n\n    if visualize:\n        diagram.ForcedPublish(context)\n\n    query_object = scene_graph.get_query_output_port().Eval(scene_graph_context)\n\n    pcd_sdf = np.inf\n    for pt in pcd.xyzs().T:\n        distances = query_object.ComputeSignedDistanceToPoint(pt)\n        for body_index in range(len(distances)):\n            distance = distances[body_index].distance\n            if distance < pcd_sdf:\n                pcd_sdf = distance\n\n    return pcd_sdf\n\n\ndef check_collision(pcd, X_G, visualize=False):\n    sdf = compute_sdf(pcd, X_G, visualize)\n    return sdf > 0","block_group":"51253bf5b5c34bb191ab7eca277c7b75","execution_count":3,"outputs":[{"name":"stderr","text":"WARNING:drake:warning: Warning: In vtkGLTFDocumentLoaderInternals.cxx, line 1354: vtkGLTFDocumentLoader (0x560e6694cd60): glTF extension KHR_texture_basisu is used in this model, but not supported by this loader. The extension will be ignored.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/e80250b0-54b1-40d4-8507-c3bb3cd021c0","content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"ZUg7IbDmIyeo","cell_id":"54ea9445f55648f8be5871489b1c9a6e","deepnote_cell_type":"markdown"},"source":"## Grasp Candidate based on Local Curvature \n\nThis is an implementation-heavy assignment, where we will implement a variation of the grasp candidate sampling algorithm on [this paper](https://arxiv.org/pdf/1706.09911.pdf). from 2017. Parts of the [library](https://github.com/atenpas/gpg) based on the paper, which the authors have named \"Grasp Pose Generator\" (GPG), is used in real grasp selection systems including the one being run at Toyota Research Institute. \n\nAs opposed to sampling candidate grasp poses using the \"antipodal heuristic\", this sampling algorithm uses a heuristic based on the local curvature. This heursitic can work quite well especially for smoother / symmetrical objects which has relatively consistent curvature characteristics. \n","block_group":"7072517611064ceb833e5bc0b63d10ae"},{"cell_type":"markdown","metadata":{"id":"1PAeSQRiHRNi","cell_id":"ccb3048213f245419a1b8b7fff05a2e9","deepnote_cell_type":"markdown"},"source":"## Computing the Darboux Frame\n\nFirst, let's work on formalizing our notion of a \"local curvature\" by bringing up the [**Darboux Frame**](https://en.wikipedia.org/wiki/Darboux_frame) from differential geometry. It has a fancy French name (after its creator), but the concept is quite simple.\n\nGiven a point $p\\in\\mathbb{R}^3$ on a differentiable surface $\\mathcal{S}\\subset\\mathbb{R}^3$, we've seen that we can compute the normal vector at point $p$. Let's denote this vector as $n(p)$. \n\nThe Darboux frame first aligns the $y$-axis with the inward normal vector, and aligns the $x$ and $z$ axis with principal axii of the tangent surface given the curvature. We will define the axis as \n- x-axis: aligned with the major axis of curvature at point $p$.\n- y-axis: aligned with the inward normal vector at point $p$.\n- z-axis: aligned with the minor axis of curvature at point $p$. \n\nWhere major axis of curvature has a smaller radius of curvature compared to the minor axis. The below figure might clear things up. \n\n<img src=\"https://raw.githubusercontent.com/RussTedrake/manipulation/master/book/figures/exercises/darboux_frame.png\" width=\"400\">","block_group":"177f1e93b74947888625c20743251ab1"},{"cell_type":"markdown","metadata":{"id":"J5WdmM8hQkQ7","cell_id":"64fb92fabba24aa4a930d5b17b4f7a16","deepnote_cell_type":"markdown"},"source":"Below, your job is to compute the RigidTransform from the world to the Darboux frame of a specific point on the pointcloud. \n\nHere is a simple outline of the algorithm that we've seeen in class:\n1. Compute the set of points $\\mathcal{S}$ around the given point using [`kdtree.query`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.query.html), with `ball_radius` as the distance upper bound.  \n2. Compute the $3\\times3$ matrix with sum of outer-products of the normal vectors. \n$$\\mathbf{N}=\\sum_{p\\in\\mathcal{S}} n(p)n^T(p)$$\n3. Run eigen decomposition and get the eigenvectors using `np.linalg.eig`. Denote the eigen vectors as $[v_1, v_2, v_3]$, in order of decreasing corresponding eigenvalues. Convince yourself that:\n- $v_1$ is the normal vector,\n- $v_2$ is the major tangent vector, \n- $v_3$ is the minor tangent vector. \nNote that `np.linalg.eig` does **not** necessarily return the eigenvectors in the correct order. (The function `np.argsort` may come in handy.)\n4. If $v_1$ is heading outwards (same direction as $n(p)$), negate $v_1$. (You can check this using the dot product.)\n5. Using $v_1,v_2,v_3$, construct the Rotation matrix by horizontally stacking the vertical vectors: $\\mathbf{R} = [v_2 | v_1 | v_3]$\n6. If the rotation is improper, negate $v_2$. (You can check this by checking the sign of the determinant.)\n5. Return a `RigidTransform` that has the rotation set as defined in the figure above, and translation defined at the desired point.\n\nThe [textbook example on normal estimation](https://manipulation.csail.mit.edu/clutter.html#normal_estimation) may be useful to reference in this problem.\n\nNOTE: Convince yourself of the following: if you knew the orthonormal basis vectors of a frame ${}^W[i,j,k]$, then the Rotation matrix of of that frame with respect to the world ${}^W\\mathbf{R}^F$ can be computed by horizontally stacking the vertical vectors ($[i|j|k]$). Why would this be? (This doesn't necessarily mean the eigenvector matrix is always a rotation matrix due to improper rotations)","block_group":"f772dded03104508a3ffcfd2c0d48eae"},{"cell_type":"code","metadata":{"id":"WRuwFwcuTQtw","source_hash":"1a5a5ba6","execution_start":1742050104983,"execution_millis":1,"execution_context_id":"4e161541-39d1-49dc-94e1-3fe79f9acfd5","cell_id":"e66df7052020424bb3d5d206432baa82","deepnote_cell_type":"code"},"source":"def compute_darboux_frame(index, pcd, kdtree, ball_radius=0.002, max_nn=50):\n    \"\"\"\n    Given a index of the pointcloud, return a RigidTransform from world to the\n    Darboux frame at that point.\n    Args:\n    - index (int): index of the pointcloud.\n    - pcd (PointCloud object): pointcloud of the object.\n    - kdtree (scipy.spatial.KDTree object): kd tree to use for nn search.\n    - ball_radius (float): ball_radius used for nearest-neighbors search\n    - max_nn (int): maximum number of points considered in nearest-neighbors search.\n    \"\"\"\n    points = pcd.xyzs()  # 3xN np array of points\n    normals = pcd.normals()  # 3xN np array of normals\n\n    # Fill in your code here.\n    p = points[:,index]\n    n_p = normals[:, index]\n    distances, indices = kdtree.query(\n            p, k=max_nn, distance_upper_bound=ball_radius\n        )\n\n    N = np.zeros((3, 3))\n    for i in indices:\n        # kdtree.query give index = N(dim) for invalid\n        if i != len(points[0]):\n            N = N + np.outer(normals[:, i], normals[:, i])\n    \n    eigenvalues, eigenvectors = np.linalg.eigh(N)\n    # Denote the eigen vectors as [v1 ​ ,v2 ​ ,v3], in order of decreasing corresponding eigenvalues\n    v1 = eigenvectors[:, 2]\n    v2 = eigenvectors[:, 1]\n    v3 = eigenvectors[:, 0]\n\n    # The dot product between two vectors is negative when the angle between the vectors is between\n    # 90 degrees and 270 degrees, excluding 90 and 270 degrees.\n    if v1 @ n_p > 0:\n        v1 = -v1\n    R = np.column_stack((v2, v1, v3))\n\n    if np.linalg.det(R) < 0:\n        R = R @ np.diag([-1, 1, 1])\n\n    R_WF = RotationMatrix(R)\n    X_WF = RigidTransform(R_WF, p)  \n\n    return X_WF","block_group":"9ca67399fd1c4e168faeca880dc4e18d","execution_count":4,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"O3Nmr31JUZPB","cell_id":"73885b777b6f4b8d9164748021ecad26","deepnote_cell_type":"markdown"},"source":"You can check your work by running the cell below and looking at the frame visualization in Meshcat. ","block_group":"aa0a41a9536f4135b3c014604ff0431d"},{"cell_type":"code","metadata":{"id":"TcNpDwGiZw1n","source_hash":"ca7e0bd2","execution_start":1742050105035,"execution_millis":1,"execution_context_id":"4e161541-39d1-49dc-94e1-3fe79f9acfd5","cell_id":"ba034c27f95443e187324d5f3346e87b","deepnote_cell_type":"code"},"source":"# 151, 11121 are pretty good verifiers of the implementation.\nindex = 151\nmeshcat.Delete()\n\n# Build KD tree.\nkdtree = KDTree(pcd.xyzs().T)\nX_WP = compute_darboux_frame(index, pcd, kdtree)\nprint(X_WP.GetAsMatrix4())\nmeshcat.SetObject(\"cloud\", pcd)\nAddMeshcatTriad(meshcat, \"frame\", length=0.025, radius=0.001, X_PT=X_WP)","block_group":"2cdbc24ebd324c38a48eaa9c041d3a43","execution_count":5,"outputs":[{"name":"stdout","text":"[[-0.97086948 -0.23047931 -0.06551145  0.00317953]\n [ 0.23890699 -0.95205443 -0.19109114  0.00916008]\n [-0.01832791 -0.2011757   0.9793837   0.18613558]\n [ 0.          0.          0.          1.        ]]\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/034c5a3b-b556-4f2b-a81d-744c7fd29372","content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"HrdDJyHzU4W_","cell_id":"90132e55bb304b54b9c18d8a0c39f809","deepnote_cell_type":"markdown"},"source":"## Collision Line Search \n\nNow we wish to align our gripper frame with the Darboux frame that we found, but naively doing it will result in collision / being too far from the object.\n\nAn important heuristic that is used in the GPG work is that grasps are more stable when contact area is maximized. For that, we would need the gripper to be as inwards as possible towards the object but avoid collisions.\n\nTo implement this, we will use a line search along a grid along the y-axis, and find the **maximum** value of $y$ (remember that our $y$ is towards the inwards normal) that results in no-collision. \n\nWe've given you the grid you should search over, and the function `distance=compute_sdf(pcd, X_WG)` that will return the signed distance function between the set of pointclouds, and the gripper, given the transform `X_WG`. You are required to use this to detect the presence of collisions. \n\nFinally, if there is no value of $y$ that results in no collisions, you should return `np.nan` for the signed distance, and `None` for the rigid transform. ","block_group":"67eae358b29648e88ebe220e3b0a1ce8"},{"cell_type":"code","metadata":{"id":"tUgWtIoDW-x2","source_hash":"f694586b","execution_start":1742050105087,"execution_millis":1,"execution_context_id":"4e161541-39d1-49dc-94e1-3fe79f9acfd5","cell_id":"1d169cd5b61c4b80939210d927799590","deepnote_cell_type":"code"},"source":"# Compute static rotation between the frame and the gripper.\n\n\ndef find_minimum_distance(pcd, X_WG):\n    \"\"\"\n    By doing line search, compute the maximum allowable distance along the y axis before penetration.\n    Return the maximum distance, as well as the new transform.\n    Args:\n      - pcd (PointCloud object): pointcloud of the object.\n      - X_WG (Drake RigidTransform object): RigidTransform. You can expect this to be the return from compute_darboux_frame.\n    Return:\n      - Tuple (signed_distance, X_WGnew) where\n      - signed_distance (float): signed distance between gripper and object pointcloud at X_WGnew.\n      - X_WGnew: New rigid transform that moves X_WG along the y axis while maximizing the y-translation subject to no collision.\n      If there is no value of y that results in no collisions, return (np.nan, None).\n    \"\"\"\n    y_grid = np.linspace(-0.05, 0.05, 10)  # do not modify\n\n    # modify here.\n    signed_distance = np.nan\n    X_WGnew = None\n\n    for y in y_grid:\n        X_WGtest = X_WG.GetAsMatrix4()\n        #Rotation\n        R = X_WGtest[:3, :3]\n        #Translation\n        T = X_WGtest[:3, 3]\n        # scaling Y direction vector by y and adding it to the translation\n        T += R[:, 1] * y\n        X_WGtest = RigidTransform(RotationMatrix(R), T)\n        distance = compute_sdf(pcd, X_WGtest)\n        if distance > 0 and (distance < signed_distance or np.isnan(signed_distance)):\n            signed_distance = distance\n            X_WGnew = X_WGtest\n\n    return signed_distance, X_WGnew","block_group":"2c617720de414756a0098d2c809f6055","execution_count":6,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"s3E771RFXO7N","cell_id":"de970f70cafe4b17a4cfa0f2148ced16","deepnote_cell_type":"markdown"},"source":"You can check your work below by running the cell below. If the visualization results in a collision, or the gripper is excessively far from the object, your implementation is probably wrong. ","block_group":"750a87d494b7453f959403304f4108eb"},{"cell_type":"code","metadata":{"id":"NvPNcEzpWqzt","source_hash":"a72a875b","execution_start":1742050105191,"execution_millis":2661,"execution_context_id":"4e161541-39d1-49dc-94e1-3fe79f9acfd5","cell_id":"2d208225f7df45a2a67db48409fb919b","deepnote_cell_type":"code"},"source":"meshcat.Delete()\nmeshcat.SetObject(\"cloud\", pcd, point_size=0.001)\nAddMeshcatTriad(meshcat, \"frame\", length=0.025, radius=0.001, X_PT=X_WP)\nshortest_distance, X_WGnew = find_minimum_distance(pcd, X_WP)\ndraw_grasp_candidate(X_WGnew, refresh=False)","block_group":"a084be4dc7d7455da3abcda08edc2870","execution_count":7,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"dc125f6a5a1046fa80f7e5d527c2b187","deepnote_cell_type":"markdown"},"source":"## Nonempty Grasp \n\nLet's add one more heuristic: when we close the gripper, we don't want what is in between the two fingers to be an empty region. That would make our robot look not very smart! \n\nThere is a simple way to check this: let's define a volumetric region swept by the gripper's closing trajectory, and call it $\\mathcal{B}(^{W}X^{G})$. We will also call the gripper body (when fully open) as the set $\\mathcal{C}(^{W}X^G)$. If there are no object pointclouds within the set $\\mathcal{B}(^{W}X^{G})$, we can simply discard it. \n\n<img src=\"https://raw.githubusercontent.com/RussTedrake/manipulation/master/book/figures/exercises/closing_plane.png\" width=\"800\">\n\nYou're probably thinking - how do I do a rigid transform on a set? Generally it's doable if the transform is affine, the set is polytopic, etc., but there is an easier trick - we just transform all the pointclouds to the gripper frame $G$! \n\nThe function below follows these steps:\n  1. Transform the pointcloud points `pcd` from world frame to gripper frame.\n  2. For each point, check if it is within the bounding box we have provided.\n  3. If there is a point inside the set, return True. If not, return false. ","block_group":"d62eab266ab84cfb9b0806252df18994"},{"cell_type":"code","metadata":{"id":"8aUP5vcLt-Cm","source_hash":"f88eb19f","execution_start":1742050107907,"execution_millis":1,"execution_context_id":"4e161541-39d1-49dc-94e1-3fe79f9acfd5","cell_id":"08987ad9550c44ff8da31ce9813eb2ba","deepnote_cell_type":"code"},"source":"def check_nonempty(pcd, X_WG, visualize=False):\n    \"\"\"\n    Check if the \"closing region\" of the gripper is nonempty by transforming the pointclouds to gripper coordinates.\n    Args:\n      - pcd (PointCloud object): pointcloud of the object.\n      - X_WG (Drake RigidTransform): transform of the gripper.\n    Return:\n      - is_nonempty (boolean): boolean set to True if there is a point within the cropped region.\n    \"\"\"\n    pcd_W_np = pcd.xyzs()\n\n    # Bounding box of the closing region written in the coordinate frame of the gripper body.\n    # Do not modify\n    crop_min = [-0.05, 0.1, -0.00625]\n    crop_max = [0.05, 0.1125, 0.00625]\n\n    # Transform the pointcloud to gripper frame.\n    X_GW = X_WG.inverse()\n    pcd_G_np = X_GW.multiply(pcd_W_np)\n\n    # Check if there are any points within the cropped region.\n    indices = np.all(\n        (\n            crop_min[0] <= pcd_G_np[0, :],\n            pcd_G_np[0, :] <= crop_max[0],\n            crop_min[1] <= pcd_G_np[1, :],\n            pcd_G_np[1, :] <= crop_max[1],\n            crop_min[2] <= pcd_G_np[2, :],\n            pcd_G_np[2, :] <= crop_max[2],\n        ),\n        axis=0,\n    )\n\n    is_nonempty = indices.any()\n\n    if visualize:\n        meshcat.Delete()\n        pcd_G = PointCloud(pcd)\n        pcd_G.mutable_xyzs()[:] = pcd_G_np\n\n        draw_grasp_candidate(RigidTransform())\n        meshcat.SetObject(\"cloud\", pcd_G)\n\n        box_length = np.array(crop_max) - np.array(crop_min)\n        box_center = (np.array(crop_max) + np.array(crop_min)) / 2.0\n        meshcat.SetObject(\n            \"closing_region\",\n            Box(box_length[0], box_length[1], box_length[2]),\n            Rgba(1, 0, 0, 0.3),\n        )\n        meshcat.SetTransform(\"closing_region\", RigidTransform(box_center))\n\n    return is_nonempty","block_group":"dbdd339f06fe4e7fa170e04c2d9bf77f","execution_count":8,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"17f9957f8e584e729f5be36efad1ed24","deepnote_cell_type":"markdown"},"source":"The following cell demonstrates the functionality of `check_nonempty`, where we have visualized the pointclouds and $\\mathcal{B}({}^W X^G)$ from the gripper frame. ","block_group":"6f2a2ec401ef4fe58e36689d8e45ae2d"},{"cell_type":"code","metadata":{"id":"BBg0NCWd2qI8","source_hash":"d8a02358","execution_start":1742050107956,"execution_millis":4,"execution_context_id":"4e161541-39d1-49dc-94e1-3fe79f9acfd5","cell_id":"e39e1c729dd240adac9da311fa9e631b","deepnote_cell_type":"code"},"source":"# Lower and upper bounds of the closing region in gripper coordinates. Do not modify.\ncheck_nonempty(pcd, X_WGnew, visualize=True)","block_group":"a0a1172da8df4856822ac28de7fe233b","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"np.False_"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/673d024b-3be1-4308-b499-4b144b254e9e","content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"dwGE2JDnXmY5","cell_id":"c202044764364551a21da8868ef010fb","deepnote_cell_type":"markdown"},"source":"## Grasp Sampling Algorithm\n\nThat was a lot of subcomponents, but we're finally onto the grand assembly. You will now generate `candidate_num` candidate grasps using everything we have written so far. The sampling algorithm goes as follows:\n\n1. Select a random point $p$ from the pointcloud (use `np.random.randint()`)\n2. Compute the Darboux frame ${}^WX^F(p)$ of the point $p$ using `compute_darboux_frame`. \n3. Randomly sample an $x$ direction translation $x\\in[x_{min},x_{max}]$, and a $z$ direction rotation $\\phi\\in[\\phi_{min},\\phi_{max}]$. Compute a grasp frame $T$ that has the relative transformation `X_FT=(RotateZ(phi),TranslateX(x))`. Convince yourself this makes the point $p$ stay in the \"closing plane\" (drawn in red) defined in the figure above. (NOTE: For ease of grading, make sure you compute the $x$ direction first with `np.random.rand()`, then compute the $\\phi$ direction with another call to `np.random.rand()`, not the other way around.) \n4. From the grasp frame $T$, translate along the $y$ axis such that the gripper is closest to the object without collision. Use `find_minimum_distance`, and call this frame $G$. Remember that `find_minimum_distance` can return `np.nan`. Skip the loop if this happens. \n5. If $G$ results in no collisions (see `check_collision`) and results in non-empty grasp (use `check_nonempty`), append it to the candidate list. If not, continue the loop until we have desired number of candidates. \n","block_group":"5fe270a264c0423cb057ffe2d27e44df"},{"cell_type":"code","metadata":{"id":"LvKVHqv8fnq1","source_hash":"7cbc2c87","execution_start":1742050108008,"execution_millis":0,"execution_context_id":"4e161541-39d1-49dc-94e1-3fe79f9acfd5","cell_id":"5e9dbc06bbb34aa5b9b7642fcffedafd","deepnote_cell_type":"code"},"source":"def compute_candidate_grasps(pcd, candidate_num=10, random_seed=5):\n    \"\"\"\n    Compute candidate grasps.\n    Args:\n        - pcd (PointCloud object): pointcloud of the object.\n        - candidate_num (int) : number of desired candidates.\n        - random_seed (int) : seed for rng, used for grading.\n    Return:\n        - candidate_lst (list of drake RigidTransforms) : candidate list of grasps.\n    \"\"\"\n\n    # Do not modify.\n    x_min = -0.03\n    x_max = 0.03\n    phi_min = -np.pi / 3\n    phi_max = np.pi / 3\n    np.random.seed(random_seed)\n\n    # Build KD tree for the pointcloud.\n    kdtree = KDTree(pcd.xyzs().T)\n    ball_radius = 0.002\n\n    candidate_count = 0\n    candidate_lst = []  # list of candidates, given by RigidTransforms.\n\n    # Modify from here.\n    while len(candidate_lst) < candidate_num:\n        # Select a random point from the pointcloud.\n        index = np.random.randint(pcd.xyzs().shape[1])\n\n        # Compute the Darboux frame of the point.\n        X_WF = compute_darboux_frame(index, pcd, kdtree)\n\n        # Randomly sample an x direction translation and a z direction rotation.\n        x = np.random.rand() * (x_max - x_min) + x_min\n        phi = np.random.rand() * (phi_max - phi_min) + phi_min\n\n        # X_FT=(RotateZ(phi),TranslateX(x))\n        X_FT = RigidTransform(RotationMatrix.MakeZRotation(phi), np.array([x, 0, 0]))\n        X_WT = X_WF @ X_FT\n\n        # X_WG RigidTransform. You can expect this to be the return from compute_darboux_frame.\n        # here our gripper in initially placed on darboux frame and the rotated around z and then translated along x\n        X_WG = X_WT\n        signed_distance, X_WGnew = find_minimum_distance(pcd, X_WG)\n        if np.isnan(signed_distance):\n            continue\n        if check_collision(pcd, X_WGnew) and check_nonempty(pcd, X_WGnew):\n            candidate_lst.append(X_WGnew)\n\n    return candidate_lst","block_group":"84039491e48a4cd7a1789dad93ad70ce","execution_count":10,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"dypaCKcOf9cn","cell_id":"4b9a58ef5d694ee49281b7f66666cecc","deepnote_cell_type":"markdown"},"source":"You can check your implementation by running the cell below. Note that although we've only sampled 20 candidates, a lot of them look promising. ","block_group":"b8c2af06cd7548398b2758bd62ebe342"},{"cell_type":"code","metadata":{"id":"ItS9GtKaZ39w","source_hash":"9beaf8b7","execution_start":1742050108059,"execution_millis":13026,"execution_context_id":"4e161541-39d1-49dc-94e1-3fe79f9acfd5","cell_id":"798eb54e90cb48a88a102994bdca951f","deepnote_cell_type":"code"},"source":"pcd_downsampled = pcd.VoxelizedDownSample(voxel_size=0.005)\n\nif running_as_notebook:\n    grasp_candidates = compute_candidate_grasps(\n        pcd_downsampled, candidate_num=3, random_seed=5\n    )\n\n    meshcat.Delete()\n    meshcat.SetObject(\"cloud\", pcd_downsampled)\n    for i in range(len(grasp_candidates)):\n        draw_grasp_candidate(\n            grasp_candidates[i], prefix=\"gripper\" + str(i), draw_frames=False\n        )","block_group":"c14b658353f84e1ab635440ffce0513c","execution_count":11,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"7jcCyk-q2U3L","cell_id":"2d7aa639ee0a4e088b86cac4b6225158","deepnote_cell_type":"markdown"},"source":"## Note on Running Time\n\nYou might be disappointed in how slowly this runs, but the same algorithm written in C++ with optimized libraries can run much faster. (I would expect around a 20 times speedup). \n\nBut more fundamentally, it's important to note how trivially parallelizable the candidate sampling process is. With a parallelized and optimized implementation, hundreds of grasp candidates can be sampled in real time.","block_group":"f755d9ca346a4fd5be4489c2f0ea89b2"},{"cell_type":"markdown","metadata":{"id":"MwE8yNg58VQN","cell_id":"f304f9b590c04de2bffa05abe5b59438","deepnote_cell_type":"markdown"},"source":"## How will this notebook be Graded?\n\nIf you are enrolled in the class, this notebook will be graded using [Gradescope](www.gradescope.com). You should have gotten the enrollement code on our announcement in Piazza. \n\nFor submission of this assignment, you must do two things. \n- Download and submit the notebook `grasp_candidate.ipynb` to Gradescope's notebook submission section, along with your notebook for the other problems.\n\nWe will evaluate the local functions in the notebook to see if the function behaves as we have expected. For this exercise, the rubric is as follows:\n- [4 pts] `compute_darboux_frame` must be implemented correctly.\n- [4 pts] `find_minimum_distance` must be implemented correctly.\n- [4 pts] `compute_candidate_grasps` must be implemented correctly.","block_group":"c81546d829544f4f95b7fca849d75dc8"},{"cell_type":"code","metadata":{"id":"xj5nAh4g8VQO","source_hash":"d76a5e83","execution_start":1742050123712,"execution_millis":25583,"execution_context_id":"4e161541-39d1-49dc-94e1-3fe79f9acfd5","cell_id":"efd52f7b06eb4188a83d37d39b4a4bfc","deepnote_cell_type":"code"},"source":"from manipulation.exercises.clutter.test_grasp_candidate import TestGraspCandidate\nfrom manipulation.exercises.grader import Grader\n\nGrader.grade_output([TestGraspCandidate], [locals()], \"results.json\")\nGrader.print_test_results(\"results.json\")","block_group":"845c5108bbd9441898ad7291b7bea469","execution_count":13,"outputs":[{"name":"stdout","text":"Total score is 12/12.\n\nScore for Test compute_candidate_grasps is 4/4.\n\nScore for Test compute_darboux_frame is 4/4.\n\nScore for Test find_minimum_distance is 4/4.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/c4824779-bc7f-401f-8be8-189e2bb78f98","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"b623e53d","execution_start":1742050149347,"execution_millis":1,"execution_context_id":"4e161541-39d1-49dc-94e1-3fe79f9acfd5","cell_id":"85dfc7a7e4c64e69957b2d0a6b94b9b2","deepnote_cell_type":"code"},"source":"","block_group":"1d90d6c5cb5a415c81d2c67d629d3578","execution_count":13,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=533dba22-ab7f-4205-8f29-33667a0de43c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"1eedbce99e334a82b7b494a46ffe7ebf"}}